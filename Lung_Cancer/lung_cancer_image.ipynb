{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d12cda8c",
      "metadata": {
        "id": "d12cda8c"
      },
      "source": [
        "# **ü´Å Lung Cancer Image Classification - Preprocessing Pipeline**\n",
        "\n",
        "## üìã Project Overview\n",
        "**Goal:** Build a deep learning model to classify lung CT scan images as **Normal** or **Malignant** (cancerous)\n",
        "\n",
        "**This Notebook Covers:**\n",
        "- ‚úÖ Data loading and preprocessing\n",
        "- ‚úÖ Image enhancement using CLAHE\n",
        "- ‚úÖ Data augmentation strategies\n",
        "- ‚úÖ Dataset splitting (train/validation/test)\n",
        "- ‚úÖ Data visualization\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Why Preprocessing Matters?\n",
        "Medical images often have:\n",
        "- Low contrast (hard to see differences)\n",
        "- Varying sizes and orientations\n",
        "- Different brightness levels\n",
        "\n",
        "Proper preprocessing helps the model learn better patterns!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "330ff478",
      "metadata": {
        "id": "330ff478"
      },
      "source": [
        "## üì¶ Import Libraries\n",
        "\n",
        "**What each library does:**\n",
        "\n",
        "- **`torch`** - PyTorch deep learning framework\n",
        "- **`torchvision`** - Image processing tools for PyTorch (datasets, transforms, models)\n",
        "- **`cv2` (OpenCV)** - Computer vision library (we use it for CLAHE enhancement)\n",
        "- **`PIL` (Python Imaging Library)** - Basic image loading and manipulation\n",
        "- **`numpy`** - Numerical operations on arrays\n",
        "- **`matplotlib/seaborn`** - Data visualization\n",
        "- **`tqdm`** - Progress bars for loops (makes waiting less boring!)\n",
        "- **`pandas`** - Data analysis (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5bef74c",
      "metadata": {
        "id": "e5bef74c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e5a5fb3",
      "metadata": {
        "id": "6e5a5fb3"
      },
      "source": [
        "## ‚öôÔ∏è Configuration Constants\n",
        "\n",
        "**Why these specific values?**\n",
        "\n",
        "- **`DATA_DIR`** - Path to your organized dataset folders (train/val/test)\n",
        "  \n",
        "- **`BATCH_SIZE = 32`** - Number of images processed together\n",
        "  - **Why 32?** Good balance between:\n",
        "    - **Memory usage** (32 images fit in most GPUs)\n",
        "    - **Training speed** (processes 32 at once, faster than 1 at a time)\n",
        "    - **Gradient stability** (averages over 32 samples reduces noise)\n",
        "  - Common choices: 16, 32, 64, 128\n",
        "  \n",
        "- **`IMAGE_SIZE = 224`** - Standard input size for most pretrained models\n",
        "  - **Why 224?** Most ImageNet pretrained models (ResNet, VGG, EfficientNet) expect 224√ó224 images\n",
        "  - Using standard size lets us use transfer learning later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9e7e9ff",
      "metadata": {
        "id": "d9e7e9ff"
      },
      "outputs": [],
      "source": [
        "# CONSTANTS\n",
        "\n",
        "# DATA_DIR = \"F:/Machine Learning/PyTorch/Lung_Cancer/Final_Split_Data\"\n",
        "DATA_DIR = \"/content/drive/MyDrive/Final_Split_Data\"\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 224 # 224x224 image pixels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcdb9c97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bcdb9c97",
        "outputId": "e414c6a8-1368-4a22-f003-a9924b3ed787"
      },
      "outputs": [],
      "source": [
        "DATA_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b6e3a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2b6e3a4",
        "outputId": "827fe690-8acf-41a0-a552-abcddb118651"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38e2d3c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38e2d3c7",
        "outputId": "9d665501-680e-45a5-97ee-0dfaf8702b3d"
      },
      "outputs": [],
      "source": [
        "print(f\"üñ•Ô∏è  Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"‚ö° CUDA Available: {torch.cuda.is_available()}\")\n",
        "print(f\"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Test actual speed\n",
        "import time\n",
        "x = torch.randn(1000, 1000).cuda()\n",
        "start = time.time()\n",
        "y = x @ x\n",
        "torch.cuda.synchronize()\n",
        "print(f\"‚è±Ô∏è  GPU Speed Test: {(time.time()-start)*1000:.2f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8db0a31c",
      "metadata": {
        "id": "8db0a31c"
      },
      "source": [
        "## üîç Custom CLAHE Transform\n",
        "\n",
        "### What is CLAHE?\n",
        "**CLAHE** = Contrast Limited Adaptive Histogram Equalization\n",
        "\n",
        "### Why do we need it?\n",
        "- Medical images (CT scans) often have **low contrast**\n",
        "- Hard to see subtle differences between normal and cancerous tissue\n",
        "- CLAHE **enhances local contrast** without over-amplifying noise\n",
        "\n",
        "### How CLAHE works:\n",
        "1. **Divides image into small tiles** (8√ó8 grid)\n",
        "2. **Applies histogram equalization to each tile separately** (enhances local details)\n",
        "3. **Limits contrast amplification** (`clip_limit=2.0` prevents noise explosion)\n",
        "4. **Blends tile boundaries smoothly** (avoids checkerboard effect)\n",
        "\n",
        "### Key Parameters:\n",
        "- **`clip_limit=2.0`** - Controls maximum contrast enhancement\n",
        "  - Lower = less enhancement (0.5-1.0 for subtle)\n",
        "  - Higher = more enhancement (2.0-4.0 for aggressive)\n",
        "  - We use 2.0 as a balanced middle ground\n",
        "  \n",
        "- **`tile_grid_size=(8, 8)`** - Divides image into 8√ó8 = 64 tiles\n",
        "  - Smaller tiles (4√ó4) = more local enhancement\n",
        "  - Larger tiles (16√ó16) = more global enhancement\n",
        "\n",
        "### Why LAB Color Space for RGB images?\n",
        "- LAB separates **luminance (L)** from **color (A, B)**\n",
        "- We only enhance luminance channel ‚Üí preserves original colors\n",
        "- Converts: RGB ‚Üí LAB ‚Üí Enhance L ‚Üí RGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf3bdb12",
      "metadata": {
        "id": "cf3bdb12"
      },
      "outputs": [],
      "source": [
        "class ApplyCLAHE:\n",
        "    def __init__(self, clip_limit=1, tile_grid_size=(8, 8)):\n",
        "        self.clip_limit = clip_limit\n",
        "        self.tile_grid_size = tile_grid_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "\n",
        "        # convert PIL image to numpy array\n",
        "        img_np = np.array(img)\n",
        "\n",
        "        # apply CLAHE\n",
        "        clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
        "\n",
        "        # if gray scale\n",
        "        if len(img_np.shape) == 2:\n",
        "            img_clahe = clahe.apply(img_np)\n",
        "\n",
        "        # if RGB, apply to each channel\n",
        "        else:\n",
        "            img_clahe = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
        "            img_clahe[:, :, 0] = clahe.apply(img_clahe[:, :, 0])\n",
        "            img_clahe = cv2.cvtColor(img_clahe, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "        return Image.fromarray(img_clahe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5474e7fb",
      "metadata": {
        "id": "5474e7fb"
      },
      "source": [
        "## üîÑ Training Data Transforms (with Augmentation)\n",
        "\n",
        "### Transform Pipeline Explained:\n",
        "\n",
        "**1. `Grayscale(num_output_channels=1)`** - Convert to grayscale\n",
        "   - **Why?** Lung CT scans don't need color, tissue structure matters more\n",
        "   - Reduces data from 3 channels (RGB) to 1 channel\n",
        "\n",
        "**2. `ApplyCLAHE(clip_limit=2.0)`** - Enhance contrast\n",
        "   - Makes tissue differences more visible\n",
        "   - Helps model detect subtle patterns\n",
        "\n",
        "**3. `Grayscale(num_output_channels=3)`** - Convert back to 3-channel\n",
        "   - **Why?** Pretrained models expect 3-channel input (RGB)\n",
        "   - Simply triplicates the grayscale channel: [G] ‚Üí [G, G, G]\n",
        "\n",
        "**4. `Resize((234, 234))` + `RandomCrop((224, 224))`** - Augmentation!\n",
        "   - Resize to slightly larger (234√ó234)\n",
        "   - Then randomly crop to 224√ó224\n",
        "   - **Why?** Each epoch sees different crops ‚Üí model learns to be position-invariant\n",
        "   - Prevents overfitting by adding variety\n",
        "\n",
        "**5. `ToTensor()`** - Convert PIL Image ‚Üí PyTorch Tensor\n",
        "   - Changes range from [0, 255] ‚Üí [0.0, 1.0]\n",
        "   - Changes shape from (H, W, C) ‚Üí (C, H, W)\n",
        "\n",
        "**6. `Normalize(mean=[0.485, 0.485, 0.485], std=[0.229, 0.229, 0.229])`**\n",
        "   - **Why these numbers?** ImageNet statistics (standard for transfer learning)\n",
        "   - Formula: `(pixel - mean) / std`\n",
        "   - Centers data around 0, makes training more stable\n",
        "   - Required if using pretrained models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7430894",
      "metadata": {
        "id": "b7430894"
      },
      "outputs": [],
      "source": [
        "# tranformers (grayscale, resize, to tensor, normalize)\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    ApplyCLAHE(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IMAGE_SIZE + 10, IMAGE_SIZE + 10)),\n",
        "    transforms.RandomCrop((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.485, 0.485], std=[0.229, 0.229, 0.229])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97631a2a",
      "metadata": {
        "id": "97631a2a"
      },
      "source": [
        "## ‚úÖ Validation/Test Transforms (Deterministic)\n",
        "\n",
        "### Key Difference from Training Transforms:\n",
        "\n",
        "**NO Random Augmentation!**\n",
        "- **Training:** Uses `RandomCrop` ‚Üí different crops each time\n",
        "- **Val/Test:** Direct `Resize` ‚Üí same image every time\n",
        "\n",
        "### Why No Augmentation for Val/Test?\n",
        "- **Consistency:** We want to evaluate model performance on same images\n",
        "- **Fair comparison:** Results should be reproducible\n",
        "- **Real-world simulation:** During deployment, you'll use raw images\n",
        "\n",
        "### Transform Pipeline:\n",
        "1. **Grayscale** ‚Üí CLAHE enhancement ‚Üí **3-channel**\n",
        "2. **Direct resize to 224√ó224** (no random crop)\n",
        "3. **ToTensor** + **Normalize** (same as training)\n",
        "\n",
        "This ensures val/test preprocessing matches training preprocessing exactly, except for randomness!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d2de4c1",
      "metadata": {
        "id": "9d2de4c1"
      },
      "outputs": [],
      "source": [
        "# Val/Test transforms (deterministic)\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    ApplyCLAHE(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  # Direct resize, no crop\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.485, 0.485], std=[0.229, 0.229, 0.229])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a33b67f8",
      "metadata": {
        "id": "a33b67f8"
      },
      "source": [
        "## üìÇ Load Datasets\n",
        "\n",
        "### What is `ImageFolder`?\n",
        "PyTorch's convenient dataset loader that expects this structure:\n",
        "```\n",
        "Final_Split_Data/\n",
        "‚îú‚îÄ‚îÄ train/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Malignant/   (all cancer images here)\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ Normal/      (all normal images here)\n",
        "‚îú‚îÄ‚îÄ val/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Malignant/\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ Normal/\n",
        "‚îî‚îÄ‚îÄ test/\n",
        "    ‚îú‚îÄ‚îÄ Malignant/\n",
        "    ‚îî‚îÄ‚îÄ Normal/\n",
        "```\n",
        "\n",
        "### How it Works:\n",
        "- **Automatically assigns labels** based on folder names\n",
        "  - Malignant = class 0 or 1\n",
        "  - Normal = class 0 or 1\n",
        "- **Applies transforms** to each image when loading\n",
        "- **Returns:** (image_tensor, label) pairs\n",
        "\n",
        "### Why Separate Datasets?\n",
        "- **Training set:** Used to learn patterns (largest split, ~70-80%)\n",
        "- **Validation set:** Tune hyperparameters, check overfitting (~10-15%)\n",
        "- **Test set:** Final evaluation, never seen during training (~10-15%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75b13b78",
      "metadata": {
        "id": "75b13b78"
      },
      "outputs": [],
      "source": [
        "# laod dataset\n",
        "\n",
        "train_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=train_transforms)\n",
        "test_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, \"test\"), transform=val_test_transforms)\n",
        "val_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, \"val\"), transform=val_test_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7072b8dc",
      "metadata": {
        "id": "7072b8dc"
      },
      "source": [
        "## üîÑ Create DataLoaders\n",
        "\n",
        "### What is a DataLoader?\n",
        "A DataLoader wraps a dataset and provides:\n",
        "- **Batching:** Groups images into batches\n",
        "- **Shuffling:** Randomizes order (for training only)\n",
        "- **Parallel loading:** Loads data in background while model trains\n",
        "- **Memory management:** Efficient data transfer to GPU\n",
        "\n",
        "### Parameter Explanations:\n",
        "\n",
        "**`batch_size=32`**\n",
        "- Processes 32 images at once\n",
        "- GPU computes gradients for all 32, then averages them\n",
        "\n",
        "**`shuffle=True` (training only)**\n",
        "- **Training:** `shuffle=True` ‚Üí random order each epoch (prevents learning order patterns)\n",
        "- **Val/Test:** `shuffle=False` ‚Üí same order (consistency)\n",
        "\n",
        "**`num_workers=2`**\n",
        "- Uses 2 CPU threads to load data in background\n",
        "- **Why 2?** Good balance for most systems\n",
        "  - 0 = single-threaded (slow, blocks training)\n",
        "  - 2-4 = parallel loading (faster, keeps GPU busy)\n",
        "  - Too many = memory overhead\n",
        "\n",
        "**`pin_memory=False`**\n",
        "- `True` = faster GPU transfer (but uses more RAM)\n",
        "- `False` = slower transfer (but safer for limited RAM)\n",
        "\n",
        "**`persistent_workers=True`**\n",
        "- Keeps workers alive between epochs\n",
        "- **Benefit:** Faster epoch transitions (no worker restart overhead)\n",
        "- **Cost:** Uses more memory\n",
        "\n",
        "### Output Explanation:\n",
        "- **Dataset sizes:** Total number of images\n",
        "- **Loader sizes:** Number of batches (images √∑ batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "189cc411",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "189cc411",
        "outputId": "0e21c8b1-e2bd-49e8-dbce-6beb0c3eceb4"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=3,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=3,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=3,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Data loaded successfully!\")\n",
        "print(\"‚úÖ Classes : \", train_dataset.classes)\n",
        "print(\"‚úÖ Dataset sizes : Train\", len(train_dataset))\n",
        "print(\"‚úÖ Dataset sizes : Validation\", len(val_dataset))\n",
        "print(\"‚úÖ Dataset sizes : Test\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7153708",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7153708",
        "outputId": "a96d49f8-56ef-4e7e-dc7e-fabb7125cfdf"
      },
      "outputs": [],
      "source": [
        "print(len(train_loader), len(val_loader), len(test_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81cb75e9",
      "metadata": {
        "id": "81cb75e9"
      },
      "source": [
        "## üõ†Ô∏è Error Handling for Corrupted Images\n",
        "\n",
        "### What does this do?\n",
        "**`LOAD_TRUNCATED_IMAGES = True`** allows PIL to load partially corrupted images\n",
        "\n",
        "### Why needed?\n",
        "- Sometimes image files get corrupted during download/transfer\n",
        "- Without this, training crashes with \"image file truncated\" error\n",
        "- With this, PIL attempts to load as much as possible\n",
        "\n",
        "### When to use:\n",
        "- Large datasets downloaded from internet\n",
        "- Medical imaging datasets (often have file issues)\n",
        "- Any dataset where you can't manually verify every image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5777e9e4",
      "metadata": {
        "id": "5777e9e4"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b4dd0e3",
      "metadata": {
        "id": "3b4dd0e3"
      },
      "source": [
        "## üìä Visualize Class Distribution\n",
        "\n",
        "### Why check class distribution?\n",
        "\n",
        "**Class Imbalance Problem:**\n",
        "- If dataset has 900 Normal and 100 Malignant images\n",
        "- Model might just predict \"Normal\" for everything ‚Üí 90% accuracy!\n",
        "- But it never learned to detect cancer (terrible for medical use)\n",
        "\n",
        "### What to look for:\n",
        "- ‚úÖ **Balanced:** Both classes have similar counts (~50/50)\n",
        "- ‚ö†Ô∏è **Slightly imbalanced:** 60/40 or 70/30 (often okay)\n",
        "- ‚ùå **Severely imbalanced:** 90/10 or worse (needs special handling)\n",
        "\n",
        "### If imbalanced, solutions:\n",
        "1. **Data augmentation** (generate more samples for minority class) ‚Üê I did this offline!\n",
        "2. **Class weights** (penalize model more for minority class errors)\n",
        "3. **Oversampling/Undersampling**\n",
        "4. **Use F1-score instead of accuracy**\n",
        "\n",
        "### This Plot Shows:\n",
        "- Red bar = Malignant images count\n",
        "- Green bar = Normal images count\n",
        "- Ideally should be roughly equal!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cceb6bb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "cceb6bb5",
        "outputId": "24500dd0-a0ea-4669-fe8a-a8f67adbd275"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = train_dataset.targets\n",
        "label_counts = Counter(labels)\n",
        "\n",
        "class_names = train_dataset.classes\n",
        "class_labels = [class_names[i] for i in label_counts.keys()]\n",
        "counts = list(label_counts.values())\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(class_labels, counts, color=['red', 'green'])\n",
        "plt.title(\"Class Distribution in Training Set\")\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18fbbf6f",
      "metadata": {
        "id": "18fbbf6f"
      },
      "source": [
        "## üñºÔ∏è Visualize Sample Images\n",
        "\n",
        "### Why visualize?\n",
        "\n",
        "**Quality Control:**\n",
        "1. ‚úÖ **Verify transforms work:** Are images properly enhanced?\n",
        "2. ‚úÖ **Check labels:** Do labels match images?\n",
        "3. ‚úÖ **Spot errors:** Are there any corrupted/wrong images?\n",
        "4. ‚úÖ **Understand data:** What does the model actually see?\n",
        "\n",
        "### Function Breakdown:\n",
        "\n",
        "**`show_batch()` function:**\n",
        "- **Randomly samples** images from dataset\n",
        "- **Unnormalizes** them (reverses normalization to display properly)\n",
        "- **Displays in grid** (3 rows √ó 5 cols = 15 images)\n",
        "\n",
        "### Why Unnormalize?\n",
        "- Training images are normalized: `(pixel - 0.485) / 0.229`\n",
        "- This makes pixel values negative/strange\n",
        "- To display properly: `pixel = normalized * 0.229 + 0.485`\n",
        "- Then clamp to [0, 1] range\n",
        "\n",
        "### What to Check:\n",
        "- ‚úÖ CLAHE enhancement working? (good contrast)\n",
        "- ‚úÖ Images clearly visible?\n",
        "- ‚úÖ Labels match image content?\n",
        "- ‚úÖ Any obvious corrupted images?\n",
        "\n",
        "**Pro Tip:** Run this multiple times to see different random samples!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48231146",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "48231146",
        "outputId": "0c6f437a-cdf3-412a-aa5e-164abe2c6f24"
      },
      "outputs": [],
      "source": [
        "def show_batch(dataset, class_names, num_images=24):\n",
        "\n",
        "    # Get images directly from dataset (much faster)\n",
        "    indices = np.random.choice(len(dataset), min(num_images, len(dataset)), replace=False)\n",
        "\n",
        "    rows = 3\n",
        "    cols = 5\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 6))\n",
        "\n",
        "    # Unnormalize parameters\n",
        "    mean = torch.tensor([0.485, 0.485, 0.485]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.229, 0.229]).view(3, 1, 1)\n",
        "\n",
        "    for i, ax in enumerate(axes.flatten()):\n",
        "        if i < len(indices):\n",
        "            img, label = dataset[indices[i]]\n",
        "\n",
        "            # Unnormalize\n",
        "            img = img * std + mean\n",
        "            img = torch.clamp(img, 0, 1)\n",
        "\n",
        "            # Convert to numpy\n",
        "            img = img.numpy().transpose((1, 2, 0))\n",
        "\n",
        "            ax.imshow(img, cmap='gray')\n",
        "            ax.set_title(class_names[label], fontsize=10)\n",
        "            ax.axis('off')\n",
        "        else:\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.suptitle(\"Sample Images from Training Set\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call with dataset instead of dataloader\n",
        "show_batch(train_dataset, train_dataset.classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "266fb78c",
      "metadata": {
        "id": "266fb78c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af36dc2e",
      "metadata": {},
      "source": [
        "# üß† CNN Model Architecture\n",
        "\n",
        "## Architecture Overview:\n",
        "\n",
        "**Convolutional Blocks:**\n",
        "1. **Block 1:** Conv(32 filters) ‚Üí ReLU ‚Üí MaxPool ‚Üí Dropout(0.25)\n",
        "2. **Block 2:** Conv(64 filters) ‚Üí ReLU ‚Üí MaxPool ‚Üí Dropout(0.25)  \n",
        "3. **Block 3:** Conv(128 filters) ‚Üí ReLU ‚Üí MaxPool ‚Üí Dropout(0.3)\n",
        "\n",
        "**Fully Connected Layers:**\n",
        "4. **Flatten** ‚Üí Converts feature maps to 1D vector\n",
        "5. **Dense(512)** ‚Üí ReLU ‚Üí Dropout(0.5) ‚Üí High-level feature learning\n",
        "6. **Dense(2)** ‚Üí Output layer (Normal vs Malignant)\n",
        "\n",
        "### Why This Architecture?\n",
        "\n",
        "**Increasing Filter Depth (32 ‚Üí 64 ‚Üí 128):**\n",
        "- Early layers detect simple patterns (edges, textures)\n",
        "- Deeper layers combine patterns into complex features (tissue structures)\n",
        "- More filters = more feature detectors\n",
        "\n",
        "**MaxPooling:**\n",
        "- Reduces spatial dimensions (224√ó224 ‚Üí 112√ó112 ‚Üí 56√ó56 ‚Üí 28√ó28)\n",
        "- Makes model invariant to small translations\n",
        "- Reduces computation\n",
        "\n",
        "**Dropout Strategy:**\n",
        "- Lower dropout in conv layers (0.25-0.3) - preserve spatial features\n",
        "- Higher dropout in dense layer (0.5) - prevent overfitting on high-level features\n",
        "- Critical for medical images (limited data, high overfitting risk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44ad56e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Running on CPU (training will be slower)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "945cfd7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LungCancerCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(LungCancerCNN, self).__init__()\n",
        "        \n",
        "        # Block 1: Conv(32) ‚Üí ReLU ‚Üí MaxPool ‚Üí Dropout(0.25)\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        \n",
        "        # Block 2: Conv(64) ‚Üí ReLU ‚Üí MaxPool ‚Üí Dropout(0.25)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout2 = nn.Dropout2d(0.25)\n",
        "        \n",
        "        # Block 3: Conv(128) ‚Üí ReLU ‚Üí MaxPool ‚Üí Dropout(0.3)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout3 = nn.Dropout2d(0.3)\n",
        "        \n",
        "        # Calculate flattened size: 224 / 2 / 2 / 2 = 28\n",
        "        # So feature maps are 28x28x128\n",
        "        self.flatten_size = 28 * 28 * 128\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 512)\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        # Block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        # Block 3\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout3(x)\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout4(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = LungCancerCNN(num_classes=2).to(device)\n",
        "print(\"‚úÖ Model created successfully!\")\n",
        "print(f\"üìä Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7be433e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model summary\n",
        "print(\"=\" * 70)\n",
        "print(\"üèóÔ∏è  MODEL ARCHITECTURE\")\n",
        "print(\"=\" * 70)\n",
        "print(model)\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2097907f",
      "metadata": {},
      "source": [
        "# ‚öôÔ∏è Training Configuration\n",
        "\n",
        "## Loss Function: CrossEntropyLoss\n",
        "- Perfect for binary classification\n",
        "- Combines softmax + negative log likelihood\n",
        "- Automatically handles class probabilities\n",
        "\n",
        "## Optimizer: Adam\n",
        "- **Learning rate = 0.001** (default, good starting point)\n",
        "- Adaptive learning rates per parameter\n",
        "- Works well for medical image classification\n",
        "\n",
        "## Learning Rate Scheduler: ReduceLROnPlateau\n",
        "- Reduces LR when validation loss plateaus\n",
        "- **Factor = 0.5** (halves LR)\n",
        "- **Patience = 3** (waits 3 epochs before reducing)\n",
        "- **Min LR = 1e-6** (prevents LR from getting too small)\n",
        "- Helps model converge to better minima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff87bc38",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6, verbose=True\n",
        ")\n",
        "\n",
        "# Training parameters\n",
        "NUM_EPOCHS = 30\n",
        "EARLY_STOPPING_PATIENCE = 7\n",
        "\n",
        "print(\"‚úÖ Training configuration set!\")\n",
        "print(f\"üìä Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"‚èπÔ∏è  Early stopping patience: {EARLY_STOPPING_PATIENCE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09ba3aa2",
      "metadata": {},
      "source": [
        "# üèãÔ∏è Training & Validation Functions\n",
        "\n",
        "## Training Function:\n",
        "1. **Sets model to training mode** (`model.train()`)\n",
        "2. **Iterates through batches** with progress bar\n",
        "3. **Forward pass** ‚Üí compute loss\n",
        "4. **Backward pass** ‚Üí compute gradients\n",
        "5. **Optimizer step** ‚Üí update weights\n",
        "6. **Tracks metrics** (loss, accuracy)\n",
        "\n",
        "## Validation Function:\n",
        "1. **Sets model to evaluation mode** (`model.eval()`)\n",
        "2. **Disables gradient computation** (`torch.no_grad()`)\n",
        "3. **Evaluates on validation set**\n",
        "4. **Returns metrics** for early stopping decisions\n",
        "\n",
        "## Key Features:\n",
        "- ‚úÖ Real-time progress bars (tqdm)\n",
        "- ‚úÖ Batch-level accuracy tracking\n",
        "- ‚úÖ Loss averaging\n",
        "- ‚úÖ GPU memory efficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa714901",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'acc': f'{100 * correct / total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100 * correct / total:.2f}%'\n",
        "            })\n",
        "    \n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "print(\"‚úÖ Training functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c9f7e9",
      "metadata": {},
      "source": [
        "# üöÄ Training Loop with Early Stopping\n",
        "\n",
        "## What Happens Here:\n",
        "\n",
        "**For Each Epoch:**\n",
        "1. **Train** on training set\n",
        "2. **Validate** on validation set\n",
        "3. **Update learning rate** (scheduler)\n",
        "4. **Track best model** (save if validation loss improves)\n",
        "5. **Early stopping** (stop if no improvement for 7 epochs)\n",
        "\n",
        "## Early Stopping:\n",
        "- Prevents overfitting by stopping when model stops improving\n",
        "- **Patience = 7** means we wait 7 epochs without improvement\n",
        "- Saves training time\n",
        "- Returns best model (not last model!)\n",
        "\n",
        "## Metrics Tracked:\n",
        "- ‚úÖ Training loss & accuracy\n",
        "- ‚úÖ Validation loss & accuracy\n",
        "- ‚úÖ Learning rate changes\n",
        "- ‚úÖ Best model checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4252343a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop with early stopping\n",
        "history = {\n",
        "    'train_loss': [], 'train_acc': [],\n",
        "    'val_loss': [], 'val_acc': [],\n",
        "    'lr': []\n",
        "}\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_model_state = None\n",
        "patience_counter = 0\n",
        "\n",
        "print(\"üöÄ Starting training...\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nüìÖ Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Update learning rate\n",
        "    scheduler.step(val_loss)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    \n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['lr'].append(current_lr)\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"\\nüìä Results:\")\n",
        "    print(f\"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"   Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
        "    print(f\"   Learning Rate: {current_lr:.6f}\")\n",
        "    \n",
        "    # Check for best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        patience_counter = 0\n",
        "        print(f\"   ‚úÖ New best model! (Val Loss: {val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"   ‚è≥ No improvement ({patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "        print(f\"\\n‚èπÔ∏è  Early stopping triggered after {epoch+1} epochs!\")\n",
        "        print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
        "        break\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ Training completed!\")\n",
        "\n",
        "# Load best model\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(f\"‚úÖ Best model loaded (Val Loss: {best_val_loss:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67ec18a7",
      "metadata": {},
      "source": [
        "# üìà Visualize Training History\n",
        "\n",
        "## What to Look For:\n",
        "\n",
        "**Training vs Validation Loss:**\n",
        "- ‚úÖ Both decreasing ‚Üí model learning well\n",
        "- ‚ö†Ô∏è Val loss increases while train loss decreases ‚Üí overfitting\n",
        "- ‚ö†Ô∏è Both high and flat ‚Üí underfitting (model too simple)\n",
        "\n",
        "**Training vs Validation Accuracy:**\n",
        "- ‚úÖ Both increasing together ‚Üí good generalization\n",
        "- ‚ö†Ô∏è Large gap (train >> val) ‚Üí overfitting\n",
        "- ‚ö†Ô∏è Both low ‚Üí model not learning\n",
        "\n",
        "**Learning Rate Schedule:**\n",
        "- Shows when LR was reduced (should align with validation plateaus)\n",
        "\n",
        "## Ideal Pattern:\n",
        "- Both curves smooth and converging\n",
        "- Small gap between train and validation\n",
        "- Steady improvement over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e21745",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\n",
        "axes[0].plot(history['val_loss'], label='Val Loss', marker='s', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].set_title('üìâ Training & Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy plot\n",
        "axes[1].plot(history['train_acc'], label='Train Acc', marker='o', linewidth=2)\n",
        "axes[1].plot(history['val_acc'], label='Val Acc', marker='s', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "axes[1].set_title('üìà Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate plot\n",
        "axes[2].plot(history['lr'], marker='o', linewidth=2, color='red')\n",
        "axes[2].set_xlabel('Epoch', fontsize=12)\n",
        "axes[2].set_ylabel('Learning Rate', fontsize=12)\n",
        "axes[2].set_title('‚öôÔ∏è Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "axes[2].set_yscale('log')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä FINAL TRAINING METRICS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Best Validation Loss:     {min(history['val_loss']):.4f}\")\n",
        "print(f\"Best Validation Accuracy: {max(history['val_acc']):.2f}%\")\n",
        "print(f\"Final Train Accuracy:     {history['train_acc'][-1]:.2f}%\")\n",
        "print(f\"Final Val Accuracy:       {history['val_acc'][-1]:.2f}%\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b60ba3c",
      "metadata": {},
      "source": [
        "# üéØ Test Set Evaluation\n",
        "\n",
        "## Why Test Set?\n",
        "- **Never seen during training** (unbiased evaluation)\n",
        "- **Simulates real-world performance**\n",
        "- **Final model assessment**\n",
        "\n",
        "## Metrics We'll Calculate:\n",
        "1. **Accuracy** - Overall correctness (TP + TN) / Total\n",
        "2. **Precision** - Of predicted cancers, how many were correct? TP / (TP + FP)\n",
        "3. **Recall (Sensitivity)** - Of actual cancers, how many did we catch? TP / (TP + FN)\n",
        "4. **F1-Score** - Harmonic mean of precision and recall\n",
        "5. **Confusion Matrix** - Visual breakdown of predictions\n",
        "\n",
        "## For Medical Diagnosis:\n",
        "- **High Recall** is critical! (Don't miss cancer cases)\n",
        "- **False Negatives** are dangerous (cancer labeled as normal)\n",
        "- **False Positives** are less critical (extra screening isn't harmful)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6008c511",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "    \n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"üß™ Evaluating on test set...\\n\")\n",
        "test_preds, test_labels = evaluate_model(model, test_loader, device)\n",
        "\n",
        "# Calculate metrics\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üéØ TEST SET RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n‚úÖ Test Accuracy: {test_accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "# Classification report\n",
        "print(\"üìä Detailed Classification Report:\")\n",
        "print(\"-\" * 70)\n",
        "print(classification_report(test_labels, test_preds, \n",
        "                          target_names=train_dataset.classes,\n",
        "                          digits=4))\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee928db3",
      "metadata": {},
      "source": [
        "# üî≤ Confusion Matrix Visualization\n",
        "\n",
        "## How to Read:\n",
        "\n",
        "**Matrix Layout:**\n",
        "```\n",
        "                Predicted\n",
        "              Normal | Malignant\n",
        "Actual Normal    TN   |   FP\n",
        "    Malignant    FN   |   TP\n",
        "```\n",
        "\n",
        "**What Each Cell Means:**\n",
        "- **TN (Top-Left):** Correctly identified normal cases ‚úÖ\n",
        "- **TP (Bottom-Right):** Correctly identified cancer cases ‚úÖ\n",
        "- **FP (Top-Right):** Normal classified as cancer ‚ö†Ô∏è (False Alarm)\n",
        "- **FN (Bottom-Left):** Cancer classified as normal ‚ùå (Dangerous!)\n",
        "\n",
        "## Goal:\n",
        "- **Maximize diagonal** (TN and TP)\n",
        "- **Minimize off-diagonal** (FP and FN)\n",
        "- **Especially minimize FN** (missed cancer cases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89a77d4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Raw counts\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=train_dataset.classes,\n",
        "            yticklabels=train_dataset.classes,\n",
        "            ax=axes[0], cbar_kws={'label': 'Count'})\n",
        "axes[0].set_title('üî≤ Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('True Label', fontsize=12)\n",
        "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "# Normalized (percentages)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
        "            xticklabels=train_dataset.classes,\n",
        "            yticklabels=train_dataset.classes,\n",
        "            ax=axes[1], cbar_kws={'label': 'Percentage'})\n",
        "axes[1].set_title('üî≤ Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('True Label', fontsize=12)\n",
        "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print confusion matrix interpretation\n",
        "print(\"\\nüìä Confusion Matrix Breakdown:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"True Negatives (TN):  {cm[0][0]:4d} - Normal correctly identified\")\n",
        "print(f\"False Positives (FP): {cm[0][1]:4d} - Normal wrongly labeled as Malignant\")\n",
        "print(f\"False Negatives (FN): {cm[1][0]:4d} - Malignant wrongly labeled as Normal ‚ö†Ô∏è\")\n",
        "print(f\"True Positives (TP):  {cm[1][1]:4d} - Malignant correctly identified\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c76ba123",
      "metadata": {},
      "source": [
        "# üñºÔ∏è Visualize Predictions\n",
        "\n",
        "## What This Shows:\n",
        "- **Random sample** of test images\n",
        "- **True labels** vs **predicted labels**\n",
        "- **Correct predictions** in green ‚úÖ\n",
        "- **Incorrect predictions** in red ‚ùå\n",
        "\n",
        "## Analysis:\n",
        "- Look for patterns in errors\n",
        "- Are certain types of images harder to classify?\n",
        "- Do misclassifications make visual sense?\n",
        "- Quality control for model behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b0086b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(model, dataset, class_names, num_images=15, device='cuda'):\n",
        "    \"\"\"Visualize model predictions on sample images\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Random sample\n",
        "    indices = np.random.choice(len(dataset), num_images, replace=False)\n",
        "    \n",
        "    rows = 3\n",
        "    cols = 5\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 9))\n",
        "    \n",
        "    # Unnormalize parameters\n",
        "    mean = torch.tensor([0.485, 0.485, 0.485]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.229, 0.229]).view(3, 1, 1)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, ax in enumerate(axes.flatten()):\n",
        "            if i < len(indices):\n",
        "                img, label = dataset[indices[i]]\n",
        "                \n",
        "                # Predict\n",
        "                img_batch = img.unsqueeze(0).to(device)\n",
        "                output = model(img_batch)\n",
        "                _, predicted = torch.max(output, 1)\n",
        "                pred_class = predicted.item()\n",
        "                \n",
        "                # Unnormalize image\n",
        "                img = img * std + mean\n",
        "                img = torch.clamp(img, 0, 1)\n",
        "                img = img.numpy().transpose((1, 2, 0))\n",
        "                \n",
        "                # Display\n",
        "                ax.imshow(img, cmap='gray')\n",
        "                \n",
        "                # Color: green if correct, red if wrong\n",
        "                is_correct = (pred_class == label)\n",
        "                color = 'green' if is_correct else 'red'\n",
        "                symbol = '‚úì' if is_correct else '‚úó'\n",
        "                \n",
        "                title = f\"{symbol} True: {class_names[label]}\\nPred: {class_names[pred_class]}\"\n",
        "                ax.set_title(title, fontsize=9, color=color, fontweight='bold')\n",
        "                ax.axis('off')\n",
        "            else:\n",
        "                ax.axis('off')\n",
        "    \n",
        "    plt.suptitle(\"üñºÔ∏è Sample Predictions on Test Set\", fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "visualize_predictions(model, test_dataset, train_dataset.classes, num_images=15, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4852de6",
      "metadata": {},
      "source": [
        "# üíæ Save Model\n",
        "\n",
        "## What Gets Saved:\n",
        "\n",
        "**1. Model State Dict (lung_cancer_cnn.pth):**\n",
        "- Model weights and biases\n",
        "- Use for inference/deployment\n",
        "- Requires model architecture to load\n",
        "\n",
        "**2. Complete Checkpoint (lung_cancer_checkpoint.pth):**\n",
        "- Model state\n",
        "- Optimizer state\n",
        "- Training history\n",
        "- Hyperparameters\n",
        "- Use to resume training\n",
        "\n",
        "## File Locations:\n",
        "- Saved in the same directory as notebook\n",
        "- Can be uploaded to cloud storage\n",
        "- Use for deployment or further training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16a4199e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model\n",
        "save_dir = \".\"  # Current directory\n",
        "\n",
        "# Save model state dict\n",
        "model_path = os.path.join(save_dir, \"lung_cancer_cnn.pth\")\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"‚úÖ Model saved to: {model_path}\")\n",
        "\n",
        "# Save complete checkpoint\n",
        "checkpoint = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'history': history,\n",
        "    'test_accuracy': test_accuracy,\n",
        "    'class_names': train_dataset.classes,\n",
        "    'image_size': IMAGE_SIZE,\n",
        "    'batch_size': BATCH_SIZE\n",
        "}\n",
        "\n",
        "checkpoint_path = os.path.join(save_dir, \"lung_cancer_checkpoint.pth\")\n",
        "torch.save(checkpoint, checkpoint_path)\n",
        "print(f\"‚úÖ Checkpoint saved to: {checkpoint_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üéâ Training Pipeline Complete!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"‚úÖ Final Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "print(f\"‚úÖ Model files saved successfully\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a108977c",
      "metadata": {},
      "source": [
        "# üîÑ How to Load and Use Saved Model\n",
        "\n",
        "## Loading the Model:\n",
        "\n",
        "Use this code to load the trained model for inference or deployment:\n",
        "\n",
        "```python\n",
        "# Create model instance\n",
        "model = LungCancerCNN(num_classes=2)\n",
        "\n",
        "# Load weights\n",
        "model.load_state_dict(torch.load('lung_cancer_cnn.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Now ready for predictions!\n",
        "```\n",
        "\n",
        "## Making Predictions on New Images:\n",
        "\n",
        "```python\n",
        "from PIL import Image\n",
        "\n",
        "# Load and preprocess image\n",
        "img = Image.open('new_ct_scan.png')\n",
        "img_tensor = val_test_transforms(img).unsqueeze(0).to(device)\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    output = model(img_tensor)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    \n",
        "print(f\"Prediction: {train_dataset.classes[predicted.item()]}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1f222d2",
      "metadata": {},
      "source": [
        "# üî• Grad-CAM Visualization\n",
        "\n",
        "## What is Grad-CAM?\n",
        "**Grad-CAM** = Gradient-weighted Class Activation Mapping\n",
        "\n",
        "### Why Use Grad-CAM?\n",
        "- **Explainability:** Shows which image regions the model focuses on\n",
        "- **Trust:** Verify model looks at tissue (not artifacts/background)\n",
        "- **Debugging:** Identify if model learns spurious correlations\n",
        "- **Medical validation:** Critical for clinical applications\n",
        "\n",
        "### How Grad-CAM Works:\n",
        "1. Forward pass ‚Üí get predictions\n",
        "2. Backward pass ‚Üí compute gradients of target class w.r.t. feature maps\n",
        "3. Weight feature maps by gradients\n",
        "4. Generate heatmap showing important regions\n",
        "\n",
        "### What to Look For:\n",
        "- ‚úÖ Model focuses on lung tissue\n",
        "- ‚úÖ Different attention for Normal vs Malignant\n",
        "- ‚ùå Model focuses on borders/artifacts (bad!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2716f24",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "class GradCAM:\n",
        "    \"\"\"Grad-CAM implementation for CNN visualization\"\"\"\n",
        "    \n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        \n",
        "        # Register hooks\n",
        "        self.target_layer.register_forward_hook(self.save_activation)\n",
        "        self.target_layer.register_backward_hook(self.save_gradient)\n",
        "    \n",
        "    def save_activation(self, module, input, output):\n",
        "        \"\"\"Save forward pass activations\"\"\"\n",
        "        self.activations = output.detach()\n",
        "    \n",
        "    def save_gradient(self, module, grad_input, grad_output):\n",
        "        \"\"\"Save backward pass gradients\"\"\"\n",
        "        self.gradients = grad_output[0].detach()\n",
        "    \n",
        "    def generate_cam(self, input_image, target_class=None):\n",
        "        \"\"\"Generate Grad-CAM heatmap\"\"\"\n",
        "        # Forward pass\n",
        "        self.model.eval()\n",
        "        output = self.model(input_image)\n",
        "        \n",
        "        # If no target class specified, use predicted class\n",
        "        if target_class is None:\n",
        "            target_class = output.argmax(dim=1).item()\n",
        "        \n",
        "        # Backward pass\n",
        "        self.model.zero_grad()\n",
        "        class_score = output[0, target_class]\n",
        "        class_score.backward()\n",
        "        \n",
        "        # Generate CAM\n",
        "        gradients = self.gradients[0]  # [C, H, W]\n",
        "        activations = self.activations[0]  # [C, H, W]\n",
        "        \n",
        "        # Global average pooling on gradients\n",
        "        weights = gradients.mean(dim=(1, 2))  # [C]\n",
        "        \n",
        "        # Weighted combination of activation maps\n",
        "        cam = torch.zeros(activations.shape[1:], dtype=torch.float32)\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * activations[i]\n",
        "        \n",
        "        # ReLU (only positive contributions)\n",
        "        cam = F.relu(cam)\n",
        "        \n",
        "        # Normalize to [0, 1]\n",
        "        cam = cam - cam.min()\n",
        "        cam = cam / (cam.max() + 1e-8)\n",
        "        \n",
        "        return cam.cpu().numpy(), target_class\n",
        "\n",
        "\n",
        "def visualize_gradcam(model, dataset, class_names, num_images=9, device='cuda'):\n",
        "    \"\"\"Visualize Grad-CAM for sample images\"\"\"\n",
        "    \n",
        "    # Create Grad-CAM object (target last conv layer)\n",
        "    gradcam = GradCAM(model, target_layer=model.conv3)\n",
        "    \n",
        "    # Random sample\n",
        "    indices = np.random.choice(len(dataset), num_images, replace=False)\n",
        "    \n",
        "    rows = 3\n",
        "    cols = 3\n",
        "    fig, axes = plt.subplots(rows, cols * 3, figsize=(18, 9))\n",
        "    \n",
        "    # Unnormalize parameters\n",
        "    mean = torch.tensor([0.485, 0.485, 0.485]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.229, 0.229]).view(3, 1, 1)\n",
        "    \n",
        "    for idx, img_idx in enumerate(indices):\n",
        "        img, label = dataset[img_idx]\n",
        "        \n",
        "        # Get Grad-CAM\n",
        "        img_batch = img.unsqueeze(0).to(device)\n",
        "        cam, pred_class = gradcam.generate_cam(img_batch)\n",
        "        \n",
        "        # Unnormalize image\n",
        "        img_display = img * std + mean\n",
        "        img_display = torch.clamp(img_display, 0, 1)\n",
        "        img_display = img_display.numpy().transpose((1, 2, 0))\n",
        "        \n",
        "        # Resize CAM to match image size\n",
        "        cam_resized = cv2.resize(cam, (224, 224))\n",
        "        \n",
        "        # Get row and column for subplots\n",
        "        row = idx // cols\n",
        "        col_base = (idx % cols) * 3\n",
        "        \n",
        "        # 1. Original Image\n",
        "        axes[row, col_base].imshow(img_display, cmap='gray')\n",
        "        axes[row, col_base].set_title(f'True: {class_names[label]}', fontsize=9)\n",
        "        axes[row, col_base].axis('off')\n",
        "        \n",
        "        # 2. Grad-CAM Heatmap\n",
        "        axes[row, col_base + 1].imshow(cam_resized, cmap='jet')\n",
        "        axes[row, col_base + 1].set_title(f'Pred: {class_names[pred_class]}', fontsize=9)\n",
        "        axes[row, col_base + 1].axis('off')\n",
        "        \n",
        "        # 3. Overlay\n",
        "        axes[row, col_base + 2].imshow(img_display, cmap='gray')\n",
        "        axes[row, col_base + 2].imshow(cam_resized, cmap='jet', alpha=0.5)\n",
        "        \n",
        "        # Color: green if correct, red if wrong\n",
        "        is_correct = (pred_class == label)\n",
        "        color = 'green' if is_correct else 'red'\n",
        "        symbol = '‚úì' if is_correct else '‚úó'\n",
        "        axes[row, col_base + 2].set_title(f'{symbol} Overlay', fontsize=9, color=color)\n",
        "        axes[row, col_base + 2].axis('off')\n",
        "    \n",
        "    plt.suptitle('üî• Grad-CAM Visualization: Original | Heatmap | Overlay', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Generate Grad-CAM visualizations\n",
        "print(\"üî• Generating Grad-CAM visualizations...\\n\")\n",
        "visualize_gradcam(model, test_dataset, train_dataset.classes, num_images=9, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd184008",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced: Compare Grad-CAM for Correct vs Incorrect Predictions\n",
        "\n",
        "def visualize_gradcam_comparison(model, dataset, class_names, device='cuda'):\n",
        "    \"\"\"Compare Grad-CAM for correct and incorrect predictions\"\"\"\n",
        "    \n",
        "    gradcam = GradCAM(model, target_layer=model.conv3)\n",
        "    \n",
        "    # Find correct and incorrect predictions\n",
        "    correct_indices = []\n",
        "    incorrect_indices = []\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx in range(len(dataset)):\n",
        "            img, label = dataset[idx]\n",
        "            img_batch = img.unsqueeze(0).to(device)\n",
        "            output = model(img_batch)\n",
        "            pred = output.argmax(dim=1).item()\n",
        "            \n",
        "            if pred == label:\n",
        "                correct_indices.append(idx)\n",
        "            else:\n",
        "                incorrect_indices.append(idx)\n",
        "            \n",
        "            if len(correct_indices) >= 3 and len(incorrect_indices) >= 3:\n",
        "                break\n",
        "    \n",
        "    # Plot comparison\n",
        "    fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
        "    \n",
        "    mean = torch.tensor([0.485, 0.485, 0.485]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.229, 0.229]).view(3, 1, 1)\n",
        "    \n",
        "    # Correct predictions (top row)\n",
        "    for i, idx in enumerate(correct_indices[:3]):\n",
        "        img, label = dataset[idx]\n",
        "        img_batch = img.unsqueeze(0).to(device)\n",
        "        cam, pred_class = gradcam.generate_cam(img_batch)\n",
        "        \n",
        "        img_display = img * std + mean\n",
        "        img_display = torch.clamp(img_display, 0, 1).numpy().transpose((1, 2, 0))\n",
        "        cam_resized = cv2.resize(cam, (224, 224))\n",
        "        \n",
        "        # Original\n",
        "        axes[0, i*2].imshow(img_display, cmap='gray')\n",
        "        axes[0, i*2].set_title(f'‚úì True: {class_names[label]}', fontsize=9, color='green')\n",
        "        axes[0, i*2].axis('off')\n",
        "        \n",
        "        # Overlay\n",
        "        axes[0, i*2+1].imshow(img_display, cmap='gray')\n",
        "        axes[0, i*2+1].imshow(cam_resized, cmap='jet', alpha=0.5)\n",
        "        axes[0, i*2+1].set_title(f'Pred: {class_names[pred_class]}', fontsize=9, color='green')\n",
        "        axes[0, i*2+1].axis('off')\n",
        "    \n",
        "    # Incorrect predictions (bottom row)\n",
        "    for i, idx in enumerate(incorrect_indices[:3]):\n",
        "        img, label = dataset[idx]\n",
        "        img_batch = img.unsqueeze(0).to(device)\n",
        "        cam, pred_class = gradcam.generate_cam(img_batch)\n",
        "        \n",
        "        img_display = img * std + mean\n",
        "        img_display = torch.clamp(img_display, 0, 1).numpy().transpose((1, 2, 0))\n",
        "        cam_resized = cv2.resize(cam, (224, 224))\n",
        "        \n",
        "        # Original\n",
        "        axes[1, i*2].imshow(img_display, cmap='gray')\n",
        "        axes[1, i*2].set_title(f'‚úó True: {class_names[label]}', fontsize=9, color='red')\n",
        "        axes[1, i*2].axis('off')\n",
        "        \n",
        "        # Overlay\n",
        "        axes[1, i*2+1].imshow(img_display, cmap='gray')\n",
        "        axes[1, i*2+1].imshow(cam_resized, cmap='jet', alpha=0.5)\n",
        "        axes[1, i*2+1].set_title(f'Pred: {class_names[pred_class]}', fontsize=9, color='red')\n",
        "        axes[1, i*2+1].axis('off')\n",
        "    \n",
        "    plt.suptitle('üî• Grad-CAM: Correct ‚úì vs Incorrect ‚úó Predictions', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Compare correct vs incorrect\n",
        "print(\"\\nüîç Comparing Grad-CAM for correct vs incorrect predictions...\\n\")\n",
        "visualize_gradcam_comparison(model, test_dataset, train_dataset.classes, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a25d574b",
      "metadata": {},
      "source": [
        "# üìä Grad-CAM Interpretation Guide\n",
        "\n",
        "## What Good Grad-CAM Looks Like:\n",
        "- ‚úÖ **Focuses on lung tissue** (not borders/artifacts)\n",
        "- ‚úÖ **Different patterns** for Normal vs Malignant\n",
        "- ‚úÖ **Consistent attention** across similar cases\n",
        "- ‚úÖ **Localized hotspots** on suspicious regions\n",
        "\n",
        "## What Bad Grad-CAM Looks Like:\n",
        "- ‚ùå Focuses on image corners/edges\n",
        "- ‚ùå Highlights background/artifacts\n",
        "- ‚ùå Random scattered attention\n",
        "- ‚ùå Same pattern for all classes\n",
        "\n",
        "## Medical Insights:\n",
        "For **Malignant cases**, model should focus on:\n",
        "- Irregular tissue patterns\n",
        "- Dense nodules or masses\n",
        "- Texture abnormalities\n",
        "\n",
        "For **Normal cases**, model should recognize:\n",
        "- Regular tissue structure\n",
        "- Uniform density\n",
        "- Absence of abnormalities\n",
        "\n",
        "## Next Steps:\n",
        "If Grad-CAM shows problems:\n",
        "1. Add more data augmentation\n",
        "2. Use weighted loss (focus on tissue regions)\n",
        "3. Try attention mechanisms in architecture\n",
        "4. Use segmentation masks if available"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97242631",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
