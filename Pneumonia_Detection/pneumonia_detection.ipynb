{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f940474",
   "metadata": {},
   "source": [
    "# **üìã Pneumonia Detection with ResNet & PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e2825",
   "metadata": {},
   "source": [
    "### **Project Goal**\n",
    "To build a Deep Learning model that looks at Chest X-Ray images and predicts if a patient has **Pneumonia** or is **Normal**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf33a25",
   "metadata": {},
   "source": [
    "### **1. Imports and Setup**\n",
    "\n",
    "We begin by importing all necessary libraries for our pneumonia detection pipeline.\n",
    "\n",
    "\n",
    "**Core Libraries:**Medical imaging requires careful data handling, visualization for debugging, and robust model training infrastructure. These libraries form the foundation of professional deep learning workflows.\n",
    "\n",
    "*   **`torch`**: PyTorch's main library - provides tensor operations, neural network building blocks, and GPU acceleration.**Why These Imports Matter:**\n",
    "\n",
    "*   **`torch.nn`**: Contains neural network layers (Conv2d, Linear, etc.) and loss functions.\n",
    "\n",
    "*   **`torchvision`**: Specialized for computer vision - provides datasets, pre-trained models, and image transformations.*   **`tqdm`**: Progress bars to monitor training loops.\n",
    "\n",
    "*   **`DataLoader`**: Efficiently loads data in batches, handles shuffling, and supports multi-threaded data loading.*   **`pandas`**: Data manipulation (optional, for structured data analysis).\n",
    "\n",
    "*   **`matplotlib` & `seaborn`**: Visualization tools for plotting graphs and displaying images.\n",
    "\n",
    "**Utility Libraries:***   **`numpy`**: Numerical operations and array manipulation.\n",
    "*   **`os`**: Navigate file systems and construct file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febec470",
   "metadata": {},
   "source": [
    "Makes the chart appear directly inside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fab075",
   "metadata": {},
   "source": [
    "### **2. Configuration (Constants)**\n",
    "\n",
    "Configuration constants define the hyperparameters and paths for our project. Centralizing these values makes experimentation easier.\n",
    "\n",
    "\n",
    "**Key Parameters:****Performance Note:** These settings are optimized for Google Colab's T4 GPU. Adjust based on your hardware.\n",
    "\n",
    "*   **`IMAGE_SIZE = 128`**: All input images are resized to 128√ó128 pixels. \n",
    "\n",
    "    *   *Why fixed size?* Neural networks require uniform input dimensions. CNNs use fixed kernel sizes that expect consistent input shapes.    *   *Local:* Use absolute path like `'F:/datasets/chest_xray'`\n",
    "\n",
    "    *   *Trade-off:* Larger sizes (224√ó224) capture more detail but require more memory and compute. 128√ó128 balances speed and accuracy for medical imaging.    *   *Google Colab:* Data stored in Google Drive at `/content/drive/MyDrive/chest_xray`\n",
    "\n",
    "*   **`DATA_DIR`**: Path to your dataset folder containing train/val/test subdirectories.\n",
    "\n",
    "*   **`BATCH_SIZE = 64`**: Number of images processed simultaneously before updating model weights.\n",
    "\n",
    "    *   *Larger batches:* More stable gradients, better GPU utilization, faster training.    *   *Optimization:* 64 is optimal for T4 GPU with ~15GB memory.\n",
    "    *   *Smaller batches:* Better generalization, less memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ddbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 64  # ‚ö° Increased from 32 (2x faster!)\n",
    "DATA_DIR = '/content/drive/MyDrive/chest_xray'  # Update this path to where you uploaded the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47aa4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üñ•Ô∏è  Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"‚ö° CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Test actual speed\n",
    "import time\n",
    "x = torch.randn(1000, 1000).cuda()\n",
    "start = time.time()\n",
    "y = x @ x\n",
    "torch.cuda.synchronize()\n",
    "print(f\"‚è±Ô∏è  GPU Speed Test: {(time.time()-start)*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0876f1",
   "metadata": {},
   "source": [
    "### **3. Data Transforms (The Preprocessing Pipeline)**\n",
    "\n",
    "Before feeding images into the model, we apply a series of transformations. This pipeline ensures data consistency and prepares images for neural network processing.\n",
    "\n",
    "**Transform Steps:**\n",
    "\n",
    "1. **`Grayscale(num_output_channels=3)`**: Converts single-channel grayscale X-rays to 3-channel images.\n",
    "    *   *Why?* ResNet and most pre-trained models expect RGB images (3 channels: Red, Green, Blue).\n",
    "    *   *Technical Detail:* We duplicate the grayscale channel 3 times: [Gray] ‚Üí [Gray, Gray, Gray].\n",
    "    *   *Alternative:* Modify the first conv layer to accept 1 channel (we do this later for ResNet).\n",
    "\n",
    "2. **`Resize((IMAGE_SIZE, IMAGE_SIZE))`**: Standardizes all images to 128√ó128 pixels.\n",
    "\n",
    "    *   *Why?* CNNs require fixed input dimensions. Medical images come in various sizes.```\n",
    "    *   *Method:* Uses bilinear interpolation to maintain image quality.Ready for Neural Network!\n",
    "    ‚Üì\n",
    "\n",
    "3. **`ToTensor()`**: Converts PIL Image (pixel values 0-255) to PyTorch tensor (values 0.0-1.0).Centered values (-1.0 to 1.0)\n",
    "\n",
    "    *   *Technical:* Changes data type from uint8 to float32.    ‚Üì Normalize\n",
    "    *   *Shape:* Converts (Height, Width, Channels) ‚Üí (Channels, Height, Width) for PyTorch.PyTorch tensor (0.0-1.0)\n",
    "    ‚Üì ToTensor()\n",
    "\n",
    "4. **`Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])`**: Centers and scales pixel values.Standardized size\n",
    "\n",
    "    *   *Formula:* `output = (input - mean) / std`    ‚Üì Resize(128, 128)\n",
    "    *   *Result:* Transforms range from [0, 1] to [-1, 1].3-channel grayscale (R=G=B)\n",
    "    *   *Why?* Normalized inputs lead to:    ‚Üì Grayscale(3)\n",
    "        - Faster convergence during trainingOriginal X-ray (variable size, 0-255 pixels)\n",
    "        - More stable gradients```\n",
    "        - Better model generalization**Pipeline Visualization:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1db813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMERS (grayscale, resize, to tensor, normalize)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528555e",
   "metadata": {},
   "source": [
    "### **4. Loading Data (Creating Dataset & DataLoader)**\n",
    "\n",
    "PyTorch's data loading pipeline consists of two main components: **Datasets** (what data to load) and **DataLoaders** (how to load it efficiently).\n",
    "\n",
    "**ImageFolder - The Automatic Labeler:**\n",
    "\n",
    "*   **`ImageFolder`**: PyTorch's clever tool that automatically creates labels from folder structure.- Persistent workers eliminate process creation overhead\n",
    "\n",
    "*   **Expected Structure:**- Pinned memory accelerates data transfer\n",
    "    ```- Parallel loading prevents GPU starvation\n",
    "    chest_xray/These DataLoader settings provide ~2-3x speedup compared to basic configuration:\n",
    "    ‚îú‚îÄ‚îÄ train/**Performance Optimizations:**\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ NORMAL/       ‚Üê Label 0\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ PNEUMONIA/    ‚Üê Label 1*   **Test Set:** Final evaluation on unseen data - **never** used during training!\n",
    "    ‚îú‚îÄ‚îÄ val/*   **Validation Set:** Used during training to tune hyperparameters and check for overfitting.\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ NORMAL/*   **Training Set:** Model learns patterns from this data (typically 70-80% of data).\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ PNEUMONIA/**Train/Val/Test Split Strategy:**\n",
    "    ‚îî‚îÄ‚îÄ test/\n",
    "        ‚îú‚îÄ‚îÄ NORMAL/    - **`persistent_workers=True`**: Keeps workers alive between epochs (avoids recreation overhead).\n",
    "        ‚îî‚îÄ‚îÄ PNEUMONIA/    - **`pin_memory=True`**: Pins memory to speed up CPU‚ÜíGPU transfer (~10-20% faster).\n",
    "    ```    - **`num_workers=2`**: Uses 2 CPU threads for parallel data loading (reduces GPU idle time).\n",
    "*   **How it works:** Folder name ‚Üí Class label (alphabetically sorted).    - **`shuffle=False`** (Validation/Test): Maintains consistent evaluation order.\n",
    "*   **Transforms:** Each image passes through our preprocessing pipeline.    - **`shuffle=True`** (Training): Randomizes order to prevent memorization. *Critical for generalization!*\n",
    "    - **`batch_size=64`**: Process 64 images per iteration (optimized for GPU memory).\n",
    "\n",
    "**DataLoader - The Efficient Delivery System:***   **Key Parameters:**\n",
    "*   **Purpose:** Loads data in batches, handles shuffling, and supports parallel loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f0305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASETS\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, 'train'), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, 'val'), transform=transform)\n",
    "test_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, 'test'), transform=transform)\n",
    "\n",
    "# ‚ö° Optimized DataLoaders for speed\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=2,           # Parallel loading\n",
    "    pin_memory=True,         # Faster GPU transfer\n",
    "    persistent_workers=True  # Keep workers alive\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=2, \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=2, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Classes : \", train_dataset.classes)\n",
    "print(\"‚úÖ Dataset sizes : Train\", len(train_dataset))\n",
    "print(\"‚úÖ Dataset sizes : Validation\", len(val_dataset))\n",
    "print(\"‚úÖ Dataset sizes : Test\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e33c3b",
   "metadata": {},
   "source": [
    "### **5. Data Visualization & Class Imbalance Check**\n",
    "\n",
    "**Why Visualize Class Distribution?**\n",
    "In medical datasets, class imbalance is extremely common. A model trained on imbalanced data may develop biases.\n",
    "\n",
    "**The Class Imbalance Problem:**\n",
    "*   **Scenario:** 4000 Pneumonia images vs 1000 Normal images (4:1 ratio).\n",
    "*   **Naive Model Behavior:** Always predicting \"Pneumonia\" gives 80% accuracy!- **Severe Imbalance:** >10:1 ratio ‚Üí Consider oversampling minority class\n",
    "*   **Real Problem:** This model is useless - it can't detect healthy patients.- **Imbalanced:** One bar much taller ‚Üí Use class weights (see Section 18)\n",
    "- **Balanced:** Both bars roughly equal height ‚Üí No special handling needed\n",
    "**Our Solution Strategy:****What to Look For:**\n",
    "1. **Visualize:** Plot class distribution to quantify imbalance.\n",
    "2. **Weighted Loss:** Make the minority class \"more expensive\" to misclassify (implemented later).*   **Performance Gain:** ~1000x faster for large datasets!\n",
    "3. **Evaluation Metrics:** Use Precision, Recall, F1-Score instead of just accuracy.*   **‚úÖ Fast Method:** `dataset.targets` - accesses pre-computed labels (takes milliseconds)\n",
    "*   **‚ùå Slow Method:** `[label for _, label in dataset]` - loads all images (takes minutes)\n",
    "**Code Optimization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# FAST WAY: Access the labels directly from the list stored in memory\n",
    "# This takes 0.01 seconds instead of 10 minutes\n",
    "labels = train_dataset.targets \n",
    "label_counts = Counter(labels)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "class_labels = [class_names[i] for i in label_counts.keys()]\n",
    "counts = list(label_counts.values())\n",
    "\n",
    "# PLOTTING\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(class_labels, counts, color=['green', 'red'])\n",
    "plt.title(\"Class Distribution in Training Set\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f10094",
   "metadata": {},
   "source": [
    "### **6. Sanity Check (Visual Inspection of Training Data)**\n",
    "\n",
    "**Why This Step is Critical:**\n",
    "\n",
    "Before training, always visually inspect your data. This catches:- Only one type of X-ray angle ‚Üí limited dataset diversity\n",
    "\n",
    "- Corrupted images- Wrong labels ‚Üí need to fix dataset\n",
    "- Incorrect labels- Text/markers on images ‚Üí need better preprocessing\n",
    "- Poor image quality- Images appearing completely black/white ‚Üí normalization issue\n",
    "- Unexpected preprocessing artifacts**Red Flags to Watch For:**\n",
    "\n",
    "**What to Look For:**    - *Why needed?* Different libraries use different dimension ordering\n",
    "1. **Correct Labels:** Do \"Pneumonia\" images actually show lung opacity/consolidation?    - *Matplotlib Format:* (Height, Width, Channels) = (128, 128, 3)\n",
    "2. **Image Quality:** Are images clear or blurry? Proper contrast?    - *PyTorch Format:* (Channels, Height, Width) = (3, 128, 128)\n",
    "3. **Preprocessing:** Did resize/normalization work correctly? Any distortion?*   **Transpose Operation:** `transpose((1, 2, 0))`\n",
    "4. **Variety:** Do you see different patient positions, image qualities, disease severities?\n",
    "    - *Result:* Converts [-1, 1] back to [0, 1] for matplotlib display\n",
    "**Understanding the Visualization:**    - *Formula:* `original = (normalized * std) + mean = (pixel * 0.5) + 0.5`\n",
    "\n",
    "*   **`unnormalize`**: Reverses the normalization transform for human viewing.    - *Recall:* We normalized with mean=0.5, std=0.5 to get range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc962307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl, class_names):\n",
    "    \n",
    "    # Fetch one batch from the dataloader\n",
    "    images, labels = next(iter(dl))\n",
    "    fig, axes = plt.subplots(3, 8, figsize=(15, 6))\n",
    "\n",
    "    # Show images up to batch size\n",
    "    num_images = len(images)\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i < num_images:\n",
    "            img = images[i].numpy().transpose((1, 2, 0))\n",
    "            \n",
    "            img = (img * 0.5) + 0.5  # unnormalize \n",
    "            ax.imshow(img)\n",
    "            ax.set_title(class_names[labels[i]])\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    plt.suptitle(\"Sample Images from Training Set\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_batch(train_loader, train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2cdb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19acbd",
   "metadata": {},
   "source": [
    "### **7. Building a Custom CNN Model**\n",
    "Since we are learning, let's first build a \"Brain\" from scratch before using a pre-trained one.\n",
    "#\n",
    "**The Architecture:**\n",
    "*   **3 Convolutional Blocks:** Each block scans the image, finds patterns, and shrinks the size.\n",
    "    *   `Conv2d`: The scanner (finds edges/textures).\n",
    "    *   `BatchNorm`: The stabilizer (keeps math numbers healthy).\n",
    "    *   `ReLU`: The activator (allows complex patterns).\n",
    "    *   `MaxPool`: The compressor (shrinks image by 2x).\n",
    "*   **Classifier Head:** The final layers that make the decision.\n",
    "    *   `Flatten`: Squashes the 3D feature map into a 1D list of numbers.\n",
    "    *   `Linear`: Connects neurons to the final answer.\n",
    "    *   `Dropout`: Randomly turns off neurons to prevent overfitting (memorization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PneumoniaCNN(nn.Module):\n",
    "    def __init__(self, input_size=150):\n",
    "        super(PneumoniaCNN, self).__init__()\n",
    "\n",
    "        # Block 1 : Input (3 RGB chnannels) -> 32 filters\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2) # Image size halves: 150 -> 75\n",
    "        )\n",
    "\n",
    "        # Block 2 : 32 filters -> 64 filters\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2) # Image size halves: 75 -> 37\n",
    "        )\n",
    "\n",
    "\n",
    "        # Block 3 : 64 filters -> 128 filters\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2) # Image size halves: 37 -> 18\n",
    "        )\n",
    "\n",
    "        # üß† Dynamic Math Calculation for Flattening\n",
    "        # We need to tell the Linear layer exactly how many inputs to expect.\n",
    "        # Original Size (150) divided by 2 (three times) = 150 // 8 = 18\n",
    "        final_size = input_size // 8\n",
    "        flattened_size = 128 * final_size * final_size\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5), # drop 50% of neurons to avoid overfitting\n",
    "            nn.Linear(flattened_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # drop 50% of neurons to avoid overfitting\n",
    "            nn.Linear(256, 2) # 2 output classes: Normal, Pneumonia\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten 3D tensors to 1D\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f537c",
   "metadata": {},
   "source": [
    "### **8. Training Setup (The \"Teacher\" & \"Rules\")**\n",
    "\n",
    "*   **GPU (`cuda`):** We move the model to the graphics card for 100x faster training.\n",
    "*   **Class Imbalance Handling:**\n",
    "    *   *Problem:* We have way more \"Pneumonia\" images than \"Normal\". The model might become lazy and just guess \"Pneumonia\".\n",
    "    *   *Solution:* We calculate **Weights**. If \"Normal\" is rare, we tell the model: *\"Pay 3x more attention to Normal images during grading.\"*\n",
    "*   **Optimizer (`Adam`):** The algorithm that updates the weights. \n",
    "*   **Scheduler (`ReduceLROnPlateau`):** If the model stops learning (loss flattens), this tool lowers the Learning Rate to help it find a more precise solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# set device (GPU is must for faster training)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# initialize model with correct input size\n",
    "model = PneumoniaCNN(input_size=IMAGE_SIZE).to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "#  Handle Class Imbalance (The 'Weighted Loss' Strategy)\n",
    "# We count samples to see which class is rare\n",
    "labels = [label for _, label in train_dataset.imgs]\n",
    "class_counts = Counter(labels)\n",
    "total = sum(class_counts.values())\n",
    "\n",
    "# Calculate weights: inverse of frequency\n",
    "weights = [total / class_counts[i] for i in range(len(class_counts))]\n",
    "\n",
    "# Define loss function with weights\n",
    "class_weights = torch.FloatTensor(weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights) # scorecard\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4) # updater\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3) # learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20a4e8",
   "metadata": {},
   "source": [
    "### **9. The Advanced Training Loop (Mixed Precision)**\n",
    "This isn't just a basic loop. It uses **Automatic Mixed Precision (AMP)**.\n",
    "#\n",
    "*   **Concept:** Normal math uses `float32` (heavy). AMP intelligently switches to `float16` (light) where possible.\n",
    "*   **Benefit:** Training becomes **2x Faster** and uses **less Memory**.\n",
    "*   **`GradScaler`:** Helps manage the small numbers in `float16` so we don't lose precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "from torch import amp\n",
    "\n",
    "# manager for mixed precision\n",
    "scaler = GradScaler() \n",
    "\n",
    "def train_model(model, loader):\n",
    "    model.train() # set to training mode (enales dropout, batchnorm, etc.)\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    pbar = tqdm(loader, desc='Training', unit='batch')\n",
    "\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # reset gradients\n",
    "        \n",
    "        # ‚ö° MIXED PRECISION: 2-3x faster!\n",
    "        # runs the forward pass with mixed precision\n",
    "        with amp.autocast(device_type='cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # scales the loss, calls backward(), and unscales the gradients\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() \n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "        pbar.set_postfix({'loss': f'{(total_loss / len(loader)) * 100:.2f}%', 'accuracy': f'{(correct / len(loader.dataset)) * 100:.2f}%'})\n",
    "\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc66c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, loader):\n",
    "    model.eval() # set to evaluation mode (disables dropout, batchnorm, etc.)\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Validating\", leave=False)\n",
    "\n",
    "    with torch.no_grad(): # disable gradient calculations\n",
    "        # ‚ö° Use mixed precision for validation too\n",
    "        with amp.autocast(device_type='cuda'):\n",
    "            for images, labels in pbar:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_loss += loss.item() \n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "\n",
    "                pbar.set_postfix({'loss': f'{(total_loss / len(loader)) * 100:.2f}%', 'accuracy': f'{(correct / len(loader.dataset)) * 100:.2f}%'})\n",
    "\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3129f52b",
   "metadata": {},
   "source": [
    "### **10. Running the Experiment**\n",
    "We loop through **Epochs** (full passes of the dataset).\n",
    "*   **Checkpointing:** We save the model (`best_pneumonia_cnn.pth`) *only* when it beats its previous best score. This ensures we keep the smartest version, not the last version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43156188",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5  ## change to 25 for better results\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS + 1):\n",
    "\n",
    "    # ‚ö° Train\n",
    "    train_loss, train_accuracy = train_model(model, train_loader)\n",
    "    \n",
    "    # ‚ö° Validate every 2 epochs (except first and last)\n",
    "    if epoch % 2 == 0 or epoch == NUM_EPOCHS:\n",
    "        val_loss, val_accuracy = validate_model(model, val_loader)\n",
    "\n",
    "        # store history\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # adjust learning rate based on validation accuracy\n",
    "        scheduler.step(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{NUM_EPOCHS}  \"\n",
    "              f\"Train Loss: {train_loss*100:.2f}%, Acc: {train_accuracy*100:.2f}%  \"\n",
    "              f\"Val Loss: {val_loss*100:.2f}%, Acc: {val_accuracy*100:.2f}%\", flush=True)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_pneumonia_cnn.pth')\n",
    "            print(\"‚úÖ New Best Model Saved!\", flush=True)\n",
    "    else:\n",
    "        print(f\"Epoch {epoch}/{NUM_EPOCHS}  \"\n",
    "              f\"Train Loss: {train_loss*100:.2f}%, Acc: {train_accuracy*100:.2f}%\", flush=True)\n",
    "\n",
    "print(f\"üéØ Best validation Accuracy: {best_val_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d407ed",
   "metadata": {},
   "source": [
    "### **11. Performance Analysis**\n",
    "*   **Loss Graph:** Should go **DOWN**. If Validation Loss goes UP while Training Loss goes DOWN, you are **Overfitting**.\n",
    "*   **Accuracy Graph:** Should go **UP**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create epoch indices that match validation points\n",
    "validation_epochs = [i for i in range(NUM_EPOCHS + 1) if i % 2 == 0 or i == NUM_EPOCHS]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(validation_epochs, train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(validation_epochs, val_losses, label='Validation Loss', marker='o')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(validation_epochs, train_accuracies, label='Train Accuracy', marker='o')\n",
    "plt.plot(validation_epochs, val_accuracies, label='Validation Accuracy', marker='o')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee3772",
   "metadata": {},
   "source": [
    "### **12. Transfer Learning with ResNet18**\n",
    "\n",
    "**Why Transfer Learning?**\n",
    "Custom CNNs are good for learning, but pre-trained models are superior for real-world applications.\n",
    "\n",
    "**ResNet18 Advantages:**\n",
    "*   **Pre-trained on ImageNet:** Already learned to detect edges, textures, shapes from 1.2M images across 1000 categories.*   **Validation/Test:** NO augmentation - evaluate on original, unmodified images\n",
    "*   **Skip Connections:** Residual connections prevent vanishing gradients, enabling deeper networks.*   **Training:** Includes augmentation to create variation and prevent overfitting\n",
    "*   **Proven Architecture:** State-of-the-art on many computer vision benchmarks.**Train vs Val/Test Transforms:**\n",
    "*   **Transfer Learning:** We leverage its learned features and fine-tune for medical imaging.\n",
    "- **Extreme Crops:** May cut off diagnostic regions\n",
    "**Our Transfer Learning Strategy:**- **Large Rotations:** Unrealistic patient positioning\n",
    "1. **Load Pre-trained Weights:** Start with ImageNet knowledge (general visual features)- **Vertical Flips:** Anatomically incorrect (lungs always at top)\n",
    "2. **Modify Architecture:** Replace final layer for our 2-class problem- **Color Jittering:** X-rays are grayscale - intensity changes can hide pathology\n",
    "3. **Fine-tune:** Train on chest X-rays while keeping learned features**‚ö†Ô∏è Augmentations to AVOID for Medical Imaging:**\n",
    "\n",
    "**Data Augmentation - Teaching Robustness:**    - *Why not more?* Large rotations are unrealistic in medical imaging\n",
    "*   **Purpose:** Artificially expand training data to prevent overfitting.    - *Why small angle?* Patient positioning varies slightly\n",
    "*   **Medical Image Considerations:** Be conservative! Extreme augmentations may introduce unrealistic artifacts.*   **`RandomRotation(degrees=3)`**: Slight rotation (¬±3¬∞)\n",
    "\n",
    "**Safe Augmentations for X-rays:**    - *50% chance:* Doubles training variety without distortion\n",
    "*   **`RandomHorizontalFlip(p=0.5)`**: Mirrors image left‚Üîright    - *Medical validity:* X-rays can be from either lung/side view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05830ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 128\n",
    "\n",
    "# new augmentation pipeline\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "    transforms.RandomRotation(degrees=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è NOTE: This cell redefines datasets - only run if you want to use augmented transforms\n",
    "# Otherwise, use the datasets already loaded in cell 15\n",
    "\n",
    "# Use the DATA_DIR from earlier (Google Drive path)\n",
    "train_dir = os.path.join(DATA_DIR, 'train')\n",
    "val_dir = os.path.join(DATA_DIR, 'val')\n",
    "test_dir = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=test_val_transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=test_val_transform)\n",
    "\n",
    "BATCH_SIZE = 64  # Match the optimized batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                         num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                       num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=2, pin_memory=True)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "print(\"‚úÖ Classes : \", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ba331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of training data\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# denormalize images for visualization\n",
    "def denormalize(img):\n",
    "    img = img * 0.5 + 0.5  # unnormalize\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    return img\n",
    "\n",
    "\n",
    "# plot a grid of images\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(min(BATCH_SIZE, 16)):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(denormalize(images[i]))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c6c839",
   "metadata": {},
   "source": [
    "### **13. Loading & Modifying ResNet18**\n",
    "\n",
    "\n",
    "**Understanding the Modification:**```\n",
    "\n",
    "Output: 1000 classes     ‚Üí  Output: 2 classes\n",
    "\n",
    "ResNet18 was designed for ImageNet (1000 object categories: cats, dogs, cars, etc.). We need to adapt it for our binary classification task (Normal vs Pneumonia).  ‚Üì FC: 512‚Üí1000            ‚Üì FC: 512‚Üí2\n",
    "\n",
    "Features: 512-dim        ‚Üí  Features: 512-dim\n",
    "\n",
    "**Modification Steps:**  ‚Üì conv layers (frozen)      ‚Üì conv layers (fine-tuned)\n",
    "\n",
    "Input: RGB (3√ó224√ó224)  ‚Üí  Input: Grayscale (3√ó128√ó128)\n",
    "\n",
    "1. **Load Pre-trained Model:**ImageNet ResNet18:        Medical ResNet18:\n",
    "\n",
    "   ```python```\n",
    "\n",
    "   resnet = models.resnet18(weights=\"IMAGENET1K_V1\")**Architecture Comparison:**\n",
    "\n",
    "   ```\n",
    "\n",
    "   - Downloads model with ImageNet weights (~46MB)*   **Hybrid:** Freeze early layers, train later layers ‚Üí Balance of both\n",
    "\n",
    "   - Contains learned features from 1.2 million images*   **Fine-tuning (our approach):** Train all layers ‚Üí Better accuracy, needs more data\n",
    "\n",
    "*   **Frozen Layers:** Keep pre-trained weights ‚Üí Faster training, less data needed\n",
    "\n",
    "2. **Modify First Layer (Optional for Grayscale):****Transfer Learning Trade-off:**\n",
    "\n",
    "   ```python\n",
    "\n",
    "   resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)   - **Why 512?** ResNet18's final feature layer outputs 512-dimensional vectors\n",
    "\n",
    "   ```   - **Modified:** `Linear(512, 2)` - outputs 2 classes (Normal, Pneumonia)\n",
    "\n",
    "   - **Original:** Expects 3-channel RGB input   - **Original:** `Linear(512, 1000)` - outputs 1000 ImageNet classes\n",
    "\n",
    "   - **Modified:** Accepts 1-channel grayscale input   ```\n",
    "\n",
    "   - **Alternative:** Keep 3 channels and duplicate grayscale (our current approach)   resnet.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "   ```python\n",
    "3. **Replace Classification Head:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b30d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# load pre-trained ResNet18 model\n",
    "resnet = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "resnet.conv1 = nn.Conv2d(1, 64, kernel_size = 7, stride=2, padding=3, bias=False)\n",
    "\n",
    "num_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_features, 2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "print(\"ü§ñ ResNet18 Loaded and Modified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e689d6",
   "metadata": {},
   "source": [
    "### **14. Advanced Training: Early Stopping & Checkpointing**\n",
    "\n",
    "**The Overfitting Problem:**\n",
    "Training too long causes the model to memorize training data instead of learning general patterns. This leads to poor performance on new patients.\n",
    "\n",
    "**Our Solution: Two Guardian Mechanisms**\n",
    "\n",
    "**1. Early Stopping - The Safety Brake**\n",
    "*   **How it works:** Monitor validation accuracy each epoch. If it doesn't improve for `PATIENCE` consecutive epochs, stop training.\n",
    "*   **Logic:**\n",
    "    ```\n",
    "\n",
    "    If validation accuracy improves:*   **`best_model_wts`:** Stores parameters of best-performing model\n",
    "\n",
    "        ‚Üí Reset patience counter*   **`epochs_no_improve`:** Counts consecutive epochs without improvement\n",
    "\n",
    "        ‚Üí Save model checkpoint*   **`best_val_accuracy`:** Tracks highest validation score achieved\n",
    "\n",
    "    Else:*   **`PATIENCE = 5`:** Tolerance for non-improvement (5 consecutive bad epochs)\n",
    "\n",
    "        ‚Üí Increment patience counter*   **`NUM_EPOCHS = 25`:** Maximum training iterations (may stop early)\n",
    "\n",
    "        ‚Üí If counter >= PATIENCE: STOP!**Key Variables Explained:**\n",
    "\n",
    "    ```\n",
    "\n",
    "*   **Benefits:**```\n",
    "\n",
    "    - Prevents wasted computation on declining performance      ‚îî‚îÄ NO  ‚Üí Continue to Next Epoch\n",
    "\n",
    "    - Automatically finds optimal training duration      ‚îú‚îÄ YES ‚Üí Early Stop! Load Best Checkpoint\n",
    "\n",
    "    - Protects against overfitting    Patience >= PATIENCE?\n",
    "\n",
    "         ‚Üì\n",
    "\n",
    "*   **Setting PATIENCE:**  ‚îî‚îÄ NO  ‚Üí Increment Patience Counter\n",
    "\n",
    "    - Too small (1-2): May stop too early, missing potential improvements  ‚îú‚îÄ YES ‚Üí Save Checkpoint, Reset Patience\n",
    "\n",
    "    - Too large (10+): Wastes time, allows overfittingValidation Accuracy Improved?\n",
    "\n",
    "    - **Sweet spot: 3-5 epochs** for most medical imaging tasks  ‚Üì\n",
    "\n",
    "Evaluate on Validation Set\n",
    "\n",
    "**2. Model Checkpointing - Saving the Best Version**  ‚Üì\n",
    "\n",
    "*   **Problem:** The final epoch model isn't always the best!Train on Training Set\n",
    "\n",
    "*   **Solution:** Save model weights every time we achieve new best validation accuracy.  ‚Üì\n",
    "\n",
    "*   **File:** `best_resnet18_pneumonia.pth` contains best model state.Start Epoch\n",
    "\n",
    "```\n",
    "\n",
    "**Learning Rate Scheduling - Adaptive Optimization****Training Flow Diagram:**\n",
    "\n",
    "*   **ReduceLROnPlateau:** Automatically lowers learning rate when progress stalls.\n",
    "\n",
    "*   **Analogy:** Like slowing down a car when approaching a parking spot for precise positioning.    - `min_lr=1e-6`: Never go below this threshold\n",
    "\n",
    "*   **Settings:**    - `patience=3`: Wait 3 epochs before reducing\n",
    "\n",
    "    - `mode='max'`: Monitor increasing metric (accuracy)    - `factor=0.5`: Cut learning rate in half when stuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with early stopping and checkpointing\n",
    "\n",
    "import copy\n",
    "\n",
    "# CONFIGURATIONS\n",
    "NUM_EPOCHS = 25\n",
    "PATIENCE = 5  # for early stopping\n",
    "\n",
    "# load the best model path\n",
    "BEST_MODEL_PATH = 'best_resnet18_pneumonia.pth'\n",
    "\n",
    "# Loss, Optimizer, Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=1e-3)\n",
    "\n",
    "# Schedular : \"Reduce learning rate on plateau\"\n",
    "# if accuracy doesn't improve for 3 epochs, cut LR by half (factor = 0.5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "# logging\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "# early stopping \n",
    "best_val_accuracy = 0.0\n",
    "epochs_no_improve = 0\n",
    "best_model_wts = copy.deepcopy(resnet.state_dict())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    resnet.train()\n",
    "    total_loss, correct , total= 0, 0, 0\n",
    "\n",
    "    for images, lable in train_loader:\n",
    "        images = images.to(device)\n",
    "        lable = lable.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(images)\n",
    "        loss = criterion(outputs, lable)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.max(1)[1]\n",
    "        correct += preds.eq(lable).sum().item()\n",
    "        total += lable.size(0)\n",
    "\n",
    "\n",
    "    train_loss /= total\n",
    "    train_accuracy = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # validation phase\n",
    "    resnet.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, lable in val_loader:\n",
    "            images = images.to(device)\n",
    "            lable = lable.to(device)\n",
    "\n",
    "            outputs = resnet(images)\n",
    "            loss = criterion(outputs, lable)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = outputs.max(1)[1]\n",
    "            correct += preds.eq(lable).sum().item()\n",
    "            total += lable.size(0)\n",
    "\n",
    "    val_loss /= total\n",
    "    val_accuracy = correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    scheduler.step(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}  Train Loss: {train_loss*100:.2f}%, Acc: {train_accuracy*100:.2f}%  Val Loss: {val_loss*100:.2f}%, Acc: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "    # check for improvement\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_wts = copy.deepcopy(resnet.state_dict())\n",
    "        torch.save(resnet.state_dict(), BEST_MODEL_PATH)\n",
    "        epochs_no_improve = 0\n",
    "        print(\"üìå Best model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "        # early stopping\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(\"‚èπÔ∏è Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# load best model weights\n",
    "resnet.load_state_dict(best_model_wts)\n",
    "print(f\"‚úÖ Training completed. Best Validation Accuracy: {best_val_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ee3124",
   "metadata": {},
   "source": [
    "### **15. Visualizing Training Performance**\n",
    "\n",
    "After training, we must diagnose the model's \"health\" using graphs.\n",
    "\n",
    "*   **Loss Graph (Left):** Both lines should go **DOWN**.\n",
    "    *   *Red Flag:* If Training Loss goes down but Validation Loss goes UP, the model is **Overfitting**.\n",
    "*   **Accuracy Graph (Right):** Both lines should go **UP**.\n",
    "    *   *Goal:* We want the Orange line (Validation) to be as high as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786570b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_range, train_losses, label='Train Loss', color='blue')\n",
    "plt.plot(epoch_range, val_losses, label='Validation Loss', color='orange')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_range, train_accuracies, label='Train Accuracy', color='blue')\n",
    "plt.plot(epoch_range, val_accuracies, label='Validation Accuracy', color='orange')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724751eb",
   "metadata": {},
   "source": [
    "### **16. Deep Dive Evaluation**\n",
    "Accuracy isn't everything. In medicine, **False Negatives** (missed pneumonia) are dangerous.\n",
    "\n",
    "*   **Precision:** \"Of all the cases predicted as Pneumonia, how many were actually Pneumonia?\"\n",
    "*   **Recall (Sensitivity):** \"Of all the actual Pneumonia cases, how many did we find?\" (We want this high!).\n",
    "*   **F1-Score:** The harmonic mean of Precision and Recall.\n",
    "#\n",
    "**Confusion Matrix:**\n",
    "*   **Diagonal:** Correct predictions.\n",
    "*   **Off-Diagonal:** Mistakes. Look closely at the bottom-left box (False Negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62db6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# put model in evaluation mode\n",
    "resnet.eval()\n",
    "\n",
    "# collect all predictions and true labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = resnet(images)\n",
    "        preds = outputs.max(1)[1]\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# generate classification report\n",
    "class_names = ['Normal', 'Pneumonia']\n",
    "print(\"üìä Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "\n",
    "# plot confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61404f",
   "metadata": {},
   "source": [
    "### **17. Visualizing Model Mistakes**\n",
    "It is crucial to see *what* the model is getting wrong.\n",
    "\n",
    "*   **Correct Predictions:** Images where the model and doctor agreed.\n",
    "*   **Incorrect Predictions:** Images where the model failed.\n",
    "*   **Analysis:** Look at the incorrect images. Are they blurry? Is there a medical device obstructing the lung? This helps you understand your data quality.\n",
    "\n",
    "1. Step 1: Generate predictions\n",
    "2. Step 2: Sort into Correct vs Incorrect indices\n",
    "3. Step 3: Display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ad03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1. generate predictions on True labels\n",
    "resnet.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = resnet(images)\n",
    "        preds = outputs.max(1)[1]\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "# step 2. Find correct and incorrect predictions\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "correct_indices = np.where(all_preds == all_labels)[0]\n",
    "incorrect_indices = np.where(all_preds != all_labels)[0]\n",
    "\n",
    "\n",
    "# step 3. Visualize some correct predictions\n",
    "def show_images(indices, title, n=16):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, idx in enumerate(indices[:n]):\n",
    "        image, label = test_dataset[idx]\n",
    "        image = denormalize(image)\n",
    "\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Pred: {class_names[all_preds[idx]]}\\nTrue: {class_names[label]}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# step 4. Show correct predictions\n",
    "show_images(correct_indices, \"‚úÖ Correct Predictions\")\n",
    "show_images(incorrect_indices, \"‚ùå Incorrect Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670eecb4",
   "metadata": {},
   "source": [
    "### **18. Handling Class Imbalance (The Professional Way)**\n",
    "\n",
    "**The Problem:** If we have 4000 Pneumonia images and only 1000 Normal images, the model naturally becomes biased towards predicting \"Pneumonia\".\n",
    "\n",
    "**The Solution: Class Weighting**\n",
    "Instead of deleting data, we make the \"Normal\" images **more expensive** to get wrong.\n",
    "\n",
    "*   **`compute_class_weight`:** This function calculates the perfect penalty score. If \"Normal\" is 4x rarer, its error counts 4x more.\n",
    "*   **Weighted Loss:** We pass these weights into `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80644934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# 1. Calculate Weights automatically based on training data\n",
    "all_labels = [label for _, label in train_dataset]  # or your raw labels array\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(all_labels), y=all_labels)\n",
    "\n",
    "# 2. Convert to Tensor and move to GPU\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. update the loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.3, patience=2, min_lr=1e-6)\n",
    "\n",
    "# Early stopping setup\n",
    "best_val_acc = 0\n",
    "early_stop_counter = 0\n",
    "PATIENCE = 5\n",
    "EPOCHS = 20\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "\n",
    "    val_loss /= total\n",
    "    val_acc = correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}  Train Loss: {train_loss*100:.2f}%, Acc: {train_acc*100:.2f}%  Val Loss: {val_loss*100:.2f}%, Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    # Learning rate update\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    # Checkpoint and early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"resnet18_pneumonia_best.pth\")\n",
    "        print(\"üìå Best model saved.\")\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= PATIENCE:\n",
    "            print(\"‚èπÔ∏è Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"‚úÖ Training complete.\")\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"resnet18_pneumonia_best.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a9881",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# üìâ Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label='Train Loss', color='blue')\n",
    "plt.plot(epochs_range, val_losses, label='Val Loss', color='orange')\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# üìà Accuracy Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label='Train Accuracy', color='blue')\n",
    "plt.plot(epochs_range, val_accuracies, label='Val Accuracy', color='orange')\n",
    "plt.title(\"Accuracy Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(\"Confusion Matrix on Test Set\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f727d73a",
   "metadata": {},
   "source": [
    "### **19. Inference (Using the Model)**\n",
    "\n",
    "This is the final step: taking a raw image file from the real world (e.g., a new patient scan) and getting a diagnosis.\n",
    "\n",
    "**Key Step:** You **must** apply the exact same transforms (Resize, Normalize) to the single image that you used during training.\n",
    "\n",
    "*   **`unsqueeze(0)`**: PyTorch expects a batch of images (Batch, Channel, Height, Width). A single image is just (C, H, W). We add a fake \"Batch dimension\" of 1 to make it (1, C, H, W)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# load best model\n",
    "resnet = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "resnet.load_state_dict(torch.load(\"resnet18_pneumonia_best.pth\"))\n",
    "resnet.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "\n",
    "# predict the single image\n",
    "\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"L\")  # convert to grayscale\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # add batch dimension and move to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = resnet(image_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    class_names = ['Normal', 'Pneumonia']\n",
    "    prediction = class_names[predicted.item()]\n",
    "\n",
    "    # show image with prediction\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Prediction: {prediction}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# change path to your image\n",
    "predict_image('/content/drive/MyDrive/chest_xray/test/NORMAL/IM-0001-0001.jpeg')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
