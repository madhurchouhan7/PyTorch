{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca52459",
   "metadata": {},
   "source": [
    "# **Training Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84818039",
   "metadata": {},
   "source": [
    "- An automated **\"aseembly line\"** that chains all your preprocessing steps(Scaling, Encoding) and your model into single Object.\n",
    "- It guarantees you **never accidentally fit on your test data (prevents Data Leakage).**\n",
    "- You can call pipe.fit() once instead of running 5 different steps manually.\n",
    "- Always recommended in professional code to keep things clean, reproducible, and error-free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Data Preprocessing Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03865a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Breast Cancer Detection Dataset\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2086277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 33)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2b2b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id', 'Unnamed: 32'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ecd55a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         M        17.99         10.38          122.80     1001.0   \n",
       "1         M        20.57         17.77          132.90     1326.0   \n",
       "2         M        19.69         21.25          130.00     1203.0   \n",
       "3         M        11.42         20.38           77.58      386.1   \n",
       "4         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9bd4b",
   "metadata": {},
   "source": [
    "### **Train Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cec6490",
   "metadata": {},
   "source": [
    "- Dividing your single dataset into two separate piles: one for **studying (\"Train\")** and one for the **final exam (\"Test\").**\n",
    "- train_test_split(X, y, test_size=0.2)\n",
    "    , X = Features (Questions)\n",
    "    , y = Target (Answers)\n",
    "- To prevent Overfitting. **If the model sees the test questions during study time, it memorizes them instead of learning the logic.**\n",
    "- The very first step. **Always split before you scale or fix missing values to avoid \"Data Leakage.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into features and target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83135657",
   "metadata": {},
   "source": [
    "### **Scaling (Feature Scaling)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd31cc5",
   "metadata": {},
   "source": [
    "- **Adjusting the range of data numbers** so they are all roughly the same size (e.g., converting \"Salary: 100,000\" and \"Age: 30\" to comparable values like 1.2 and 0.5).\n",
    "- How: \n",
    "    , fit_transform(X_train): Calculate stats and scale the training data.\n",
    "    , transform(X_test): Scale the test data using the training stats.\n",
    "- Prevents the model from thinking big numbers (Salary) are more important than small numbers (Age).\n",
    "- Helps the math (Gradient Descent) run faster.\n",
    "- When features have different units (e.g., Kilograms vs. Meters) and you are using algorithms that calculate distance (KNN, SVM, Neural Networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf56ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a22171d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.7782224 , -0.13733042, -0.77630652, ..., -0.42354413,\n",
       "         0.28679742, -0.92885125],\n",
       "       [-0.11505735, -0.73724257, -0.16050763, ..., -0.66161453,\n",
       "         0.72762892, -0.38593406],\n",
       "       [ 0.47473624, -0.30738426,  0.48305938, ...,  1.03978502,\n",
       "         1.51375048,  1.25760282],\n",
       "       ...,\n",
       "       [ 0.25462189, -0.86950667,  0.23192124, ...,  0.37992431,\n",
       "         0.10912389,  0.00498997],\n",
       "       [-0.50166847, -1.60640665, -0.52761851, ..., -0.29356229,\n",
       "        -0.10374911, -0.9359482 ],\n",
       "       [-0.70485095, -1.500123  , -0.71178648, ..., -0.43778243,\n",
       "        -0.19593821,  0.23800671]], shape=(455, 30))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dccdf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515    B\n",
       "374    B\n",
       "11     M\n",
       "419    B\n",
       "87     M\n",
       "      ..\n",
       "153    B\n",
       "514    M\n",
       "227    B\n",
       "170    B\n",
       "98     B\n",
       "Name: diagnosis, Length: 455, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13f552",
   "metadata": {},
   "source": [
    "### **Label Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714fa81",
   "metadata": {},
   "source": [
    "- Converting text labels (Categories) into integers (Numbers).\n",
    "    Example: \"Cat\" → 0, \"Dog\" → 1, \"Bird\" → 2.\n",
    "- **Computers can only do math on numbers**. They cannot multiply or subtract words like \"Cat.\n",
    "- Typically used for the Target variable (y) when it is text-based (e.g., predicting \"Yes/No\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "674f46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83cd3f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df940947",
   "metadata": {},
   "source": [
    "### Numpy Arrays To PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e6cf63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting numpy arrays to torch tensors\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a58bf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7782, -0.1373, -0.7763,  ..., -0.4235,  0.2868, -0.9289],\n",
       "        [-0.1151, -0.7372, -0.1605,  ..., -0.6616,  0.7276, -0.3859],\n",
       "        [ 0.4747, -0.3074,  0.4831,  ...,  1.0398,  1.5138,  1.2576],\n",
       "        ...,\n",
       "        [ 0.2546, -0.8695,  0.2319,  ...,  0.3799,  0.1091,  0.0050],\n",
       "        [-0.5017, -1.6064, -0.5276,  ..., -0.2936, -0.1037, -0.9359],\n",
       "        [-0.7049, -1.5001, -0.7118,  ..., -0.4378, -0.1959,  0.2380]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape\n",
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "044b1fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac92d2",
   "metadata": {},
   "source": [
    "### **Defining the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd821809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastCancerNeuralNetwork():\n",
    "\n",
    "    # Initializing weights and bias\n",
    "    def __init__(self, X):\n",
    "\n",
    "        # create weight for every feature (column) in X\n",
    "        # requires_grad => This tells PyTorch: \"Watch these variables! If we make a mistake later, \n",
    "        # track back to these numbers so we can adjust them.\" This enables the magic of backpropagation.\n",
    "        self.weights = torch.rand(X.shape[1], 1, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "        # Create a single bias value, staring at 0\n",
    "        self.bias = torch.zeros(1, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    def forward(self, X):\n",
    "        z = torch.matmul(X, self.weights) + self.bias\n",
    "        y_pred = torch.sigmoid(z)\n",
    "        return y_pred\n",
    "    \n",
    "    def loss_function(self, y_pred, y_true):\n",
    "        epsilon = 1e-7  # small constant to avoid log(0)\n",
    "        y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "        loss = -torch.mean(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26eb3d",
   "metadata": {},
   "source": [
    "### Important Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82922e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6167bd8c",
   "metadata": {},
   "source": [
    "### **Training Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adef57d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.5289330566791692\n",
      "Epoch 2, Loss: 3.400511212599101\n",
      "Epoch 3, Loss: 3.268516266945045\n",
      "Epoch 4, Loss: 3.1323469581678527\n",
      "Epoch 5, Loss: 2.9927901579191687\n",
      "Epoch 6, Loss: 2.851043137117016\n",
      "Epoch 7, Loss: 2.7029609283714207\n",
      "Epoch 8, Loss: 2.554983847980423\n",
      "Epoch 9, Loss: 2.4026633470635077\n",
      "Epoch 10, Loss: 2.2494166649965517\n",
      "Epoch 11, Loss: 2.099653154900244\n",
      "Epoch 12, Loss: 1.9534812345953967\n",
      "Epoch 13, Loss: 1.813156238967321\n",
      "Epoch 14, Loss: 1.6749859201370951\n",
      "Epoch 15, Loss: 1.5459662770667604\n",
      "Epoch 16, Loss: 1.424960643495217\n",
      "Epoch 17, Loss: 1.3140122375897374\n",
      "Epoch 18, Loss: 1.2175114671229146\n",
      "Epoch 19, Loss: 1.1352769288625246\n",
      "Epoch 20, Loss: 1.0667533138418026\n",
      "Epoch 21, Loss: 1.010936168144072\n",
      "Epoch 22, Loss: 0.9663389991551633\n",
      "Epoch 23, Loss: 0.9311128862101402\n",
      "Epoch 24, Loss: 0.9033031570088161\n"
     ]
    }
   ],
   "source": [
    "# Crteating model \n",
    "model = BreastCancerNeuralNetwork(X_train_tensor)\n",
    "\n",
    "# define Loop\n",
    "for epochs in range(epochs):\n",
    "\n",
    "    # forward pass\n",
    "    y_pred = model.forward(X_train_tensor)\n",
    "\n",
    "    # loss calculation\n",
    "    loss = model.loss_function(y_pred, y_train_tensor)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # parameter update\n",
    "    with torch.no_grad():\n",
    "        model.weights -= learning_rate * model.weights.grad\n",
    "        model.bias -= learning_rate * model.bias.grad\n",
    "\n",
    "        # zero the gradients after updating\n",
    "        model.weights.grad.zero_()\n",
    "        model.bias.grad.zero_()\n",
    "\n",
    "        # print loss every each epoch\n",
    "        print(f'Epoch {epochs+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "259563fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1404], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aeb00c",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4017d0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.6140350699424744\n"
     ]
    }
   ],
   "source": [
    "# model evaluation on test data\n",
    "with torch.no_grad():\n",
    "    y_pred = model.forward(X_test_tensor)\n",
    "    y_pred = (y_pred >= 0.9).float()\n",
    "    accuracy = (y_pred.squeeze() == y_test_tensor).float().mean()\n",
    "    print(f'Accuracy on test data: {accuracy.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
